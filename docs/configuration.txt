.. highlight:: sh

.. _configuration:

=============
Configuration
=============

Since Crate Data has sensible defaults, there is no configuration
needed at all for basic operation.

Crate Data is mainly configured via a configuration file, which is
located at ``config/crate.yml``. The vanilla configuration file
distributed with the package has all available settings as comments
in it along with the according default value.

The location of the config file can be specified upon startup like
this::

    sh$ ./bin/crate -Des.config=/path/to/config.yml

Any option can be configured either by the config file or as system
property. If using system properties the required prefix 'es.' will be
ignored.

For example, configuring the cluster name by using system properties
will work this way::

    sh$ ./bin/crate -Des.cluster.name=cluster

.. highlight:: yaml

This is exactly the same as setting the cluster name in the config
file::

    cluster.name = cluster

Settings will get applied in the following order where the latter one
will overwrite the prior one:

 1. internal defaults
 2. system properties
 3. options from config file
 4. command-line properties

.. _conf_supported_settings:

Supported Settings
==================

.. _conf_hosts:

Hosts
-----

Per default Crate Data will bind itself to the ``0.0.0.0`` address.
If you want to change this behaviour, adjust the ``network.host`` setting.

This setting sets both the ``network.bind_host`` and ``network.publish_host`` values.

``network.bind_host`` is ``0.0.0.0`` by default.
This setting determines to which address crate should bind itself to.
To only bind to localhost, set it to any local address or ``_local_``.

The ``network.publish_host`` value is used
by the crate node to publish its own address to the rest of the cluster.
Per default it is the first non local address.

To explicitly bind crate to a specific interface use the interface name between underscores.
For example ``_eth0_``.

This resolves to the ip address of this interface. With ``_eth0:ipv{4,6}_`` you explicitly listen on
 an ipv6 or ipv4 address.

.. _conf_ports:

Ports
-----

Per default Crate Data will listen to the first free port in the range of ``4200-4300`` for HTTP Requests.
If there is no crate process already running on that machine, it will usually be ``4200``.

You can change the HTTP port range by adjusting the ``http.port`` setting.

For internal cluster communication crate uses TCP and listens on the first free port
in the range of ``4300-4400``.

You can change the TCP port range by adjusting the ``transport.tcp.port`` setting.

.. _conf_collecting_stats:

Collecting Stats
----------------

:stats.enabled:
    A boolean indicating whether or not to collect statistical information
    about the cluster.

:stats.jobs_log_size:
   The number of jobs kept in the ``sys.jobs_log`` table on each node for performance analytics.
   Older entries will be deleted when the ``jobs_log_size`` is reached.
   A single SQL statement results in a job to be executed on the cluster.
   A higher number results in more expressive results but also in more
   occupied RAM. Setting it to ``0`` disables collecting job information.

:stats.operations_log_size:
    The number of operations to keep in the ``sys.operations_log`` table
    on each node for performance analytics.
    Older entries will be deleted when the ``operations_log_size`` is reached.
    A job consists of one or many operations.
    A higher number results in more expressive results but also in more
    occupied RAM.
    Setting it to ``0`` disables collecting operation information.

.. _conf_graceful_stop:

Graceful Stop
-------------

:cluster.graceful_stop.min_availability:
    Options are: ``none | primaries | full``

    ``none``: No minimum data availability is required. The node may shut down
    even if records are missing after shutdown.

    ``primaries``: At least all primary shards need to be availabe after the
    node has shut down. Replicas may be missing.

    ``full``: All records and all replicas need to be available after the
    node has shut down. Data availability is full.

    .. note::

      This option is ignored if there is only 1 node in a cluster!

:cluster.graceful_stop.reallocate:
    Options are: ``true | false``

    ``true``: The ``graceful stop`` command allows shards to be reallocated before
    shutting down the node in order to ensure minimum data availability set
    with ``min_availability``.

    ``false``: The ``graceful stop`` command will fail if the cluster would
    need to reallocate shards in order to ensure the minimum data availability
    set with ``min_availability``.

    .. note::

      Make sure you have enough nodes and enough disk space for the reallocation.

:cluster.graceful_stop.timeout:
    Defines the maximum waiting time in milliseconds
    for the reallocation process to finish.
    The ``force`` setting will define the behaviour when the shutdown process
    runs into this timeout.

    The timeout expects a time value either as a long or double
    or alternatively as a string literal with a time suffix
    (``ms``, ``s``, ``m``, ``h``, ``d``, ``w``)

:cluster.graceful_stop.force:
    Options are: ``true | false``

    Defines whether ``graceful stop`` should force stopping of the node if it
    runs into the timeout which is specified with the ``timeout`` setting.

:cluster.graceful_stop.is_default:
    ``true | false``
    This options defines whether the ``stop`` command of the ``crate`` executable
    uses the graceful-stop option by default.

Discovery
---------

:discovery.zen.minimum_master_nodes:
    Set to ensure a node sees N other master eligible nodes to be
    considered operational within the cluster. It's recommended to set
    it to a higher value than 1 when running more than 2 nodes in the
    cluster. The default value is: ``1``

:discovery.zen.ping_timeout:
    Set the time to wait for ping responses from other nodes when
    discovering. Set this option to a higher value on a slow or
    congested network to minimize discovery failures. The default value
    is: ``3s``

:discovery.zen.publish_timeout:
    Time a node is waiting for responses from other nodes to a published
    cluster state. The default value is: ``30s``

.. _conf_routing:

Routing Allocation
------------------

:cluster.routing.allocation.enable:
    Options are: ``all | new_primaries``

    ``all`` allows all shard allocations. The default value ``all`` specifies
    that the cluster can allocate all kinds of shards.

    ``new_primaries`` allows allocations for new primary shards only. This means
    that for example a newly added node will not allocate any replicas. However
    it is still possible to allocate new primary shards for new indices.
    Whenever you want to perform a zero downtime upgrade of your cluster
    you need to set this value before gracefully stopping the first
    node and reset it to ``all`` after starting the last updated node.

:cluster.routing.allocation.allow_rebalance:
    Options are: ``always | indices_primary_active | indices_all_active``

    Allow to control when rebalancing will happen based on the total
    state of all the indices shards in the cluster. Defaulting to
    ``indices_all_active`` to reduce chatter during initial recovery.

:cluster.routing.allocation.cluster_concurrent_rebalance:
    Define how many concurrent rebalancing tasks are allowed cluster
    wide, defaults to ``2``.

:cluster.routing.allocation.node_initial_primaries_recoveries:
    Define the number of initial recoveries of primaries that are
    allowed per node. Since most times local gateway is used, those
    should be fast and we can handle more of those per node without
    creating load. The default value is ``4``.

:cluster.routing.allocation.node_concurrent_recoveries:
    How many concurrent recoveries are allowed to happen on a node. Defaults to ``2``.

Awareness
~~~~~~~~~

Cluster allocation awareness allows to configure shard and replicas
allocation across generic attributes associated with nodes.

.. _conf_awareness_attr:

:cluster.routing.allocation.awareness.attributes:
    Define node attributes which will be used to do awareness based
    on the allocation of a shard and its replicas.
    For example, let's say we have defined an attribute `rack_id`
    and we start 2 nodes with `node.rack_id` set to rack_one, and deploy a
    single index with 5 shards and 1 replica. The index will be fully
    deployed on the current nodes (5 shards and 1 replica each, total
    of 10 shards).
    | Now, if we start two more nodes, with node.rack_id set to
    rack_two, shards will relocate to even the number of shards across
    the nodes, but a shard and its replica will not be allocated in
    the same rack_id value.
    | The awareness attributes can hold several values

:cluster.routing.allocation.awareness.force.*.values:
   Attributes on which shard allocation will be forced. ``*`` is a placeholder
   for the awareness attribute, which can be defined using the
   `cluster.routing.allocation.awareness.attributes <conf_awareness_attr_>`_
   setting.
   Let's say we configured an awareness attribute ``zone`` and the values
   values ``zone1, zone2`` here, start 2 nodes with
   ``node.zone`` set to ``zone1`` and create an index with 5 shards
   and 1 replica. The index will be created, but only 5 shards will be
   allocated (with no replicas). Only when we start more shards with
   ``node.zone`` set to ``zone2`` the replicas will be allocated.

Balanced Shards
~~~~~~~~~~~~~~~

All these values are relative to one another. The first three are used
to compose a three separate weighting functions into one. The cluster
is balanced when no allowed action can bring the weights of each node
closer together by more then the fourth setting. Actions might not be
allowed, for instance, due to forced awareness or allocation filtering.

:cluster.routing.allocation.balance.shard:
    Defines the weight factor for shards allocated on a node
    (float). Defaults to ``0.45f``. Raising this raises the tendency to
    equalize the number of shards across all nodes in the cluster.

:cluster.routing.allocation.balance.index:
    Defines a factor to the number of shards per index allocated on a
    specific node (float). Defaults to ``0.5f``. Increasing this value
    raises the tendency to equalize the number of shards per index
    across all nodes in the cluster.

:cluster.routing.allocation.balance.primary:
    Defines a weight factor for the number of primaries of a specific
    index allocated on a node (float). Defaults to ``0.05f``. Increasing
    this value raises the tendency to equalize the number of primary shards
    across all nodes in the cluster.

:cluster.routing.allocation.balance.threshold:
    Minimal optimization value of operations that should be performed
    (non negative float). Defaults to ``1.0f``. Increasing this value will
    cause the cluster to be less aggressive about optimising the shard
    balance.

Cluster-Wide Allocation Filtering
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Allow to control the allocation of all shards based on include/exclude
filters.
E.g. this could be used to allocate all the new shards on the nodes
with specific IP addresses or custom attributes.

:cluster.routing.allocation.include.*:
    Place new shards only on nodes where one of the specified
    values matches the attribute.
    e.g.: cluster.routing.allocation.include.zone: "zone1,zone2"

:cluster.routing.allocation.exclude.*:
    Place new shards only on nodes where none of the specified
    values matches the attribute.
    e.g.: cluster.routing.allocation.exclude.zone: "zone1"

:cluster.routing.allocation.require.*:
    used to specify a number of rules, which all MUST match for
    a node in order to allocate a shard on it.
    This is in contrast to include which will include a node
    if ANY rule matches.

Disk-based Shard Allocation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

:cluster.routing.allocation.disk.threshold_enabled:
    Prevent shard allocation on nodes depending of the disk
    usage. Enabled by default.

:cluster.routing.allocation.disk.watermark.low:
    Defines the lower disk threshold limit for shard allocations.
    Defaults to ``85%``. New shards will not be allocated on nodes with
    disk usage is greater than this value. It can also be set to an
    absolute bytes value (like e.g. ``500mb``) to prevent the cluster
    from allocating new shards on node with less free disk space than
    this value.

:cluster.routing.allocation.disk.watermark.high:
   Defines the higher disk threshold limit for shard allocations.
   Defaults to ``90%``. The cluster will attempt to relocate existing
   shards to another node if the disk usage on a node rises above this
   value. It can also be set to an absolute bytes value (like
   e.g. ``500mb``) to relocate shards from nodes with less free disk
   space than this value.

By default, the cluster will retrieve information about the disk usage
of the nodes every 30 seconds. This can also be changed by setting the
`cluster.info.update.interval <conf_cluster_info_update_interval_>`_ setting.

Recovery
--------

:indices.recovery.concurrent_streams:
    Limits the number of open concurrent streams when
    recovering a shard from a peer. Defaults to ``3``.

:indices.recovery.file_chunk_size:
    Specifies the chunk size used to copy the shard data from the
    source shard. The default value is ``512kb``, it is compressed if
    ``indices.recovery.compress`` is set to ``true``.

:indices.recovery.translog_ops:
    Specifies how many transaction log lines should be transfered
    between shards in a single request during the recovery process.
    If ``indices.recovery.translog_size`` is reached first, value is
    ignored for this request.
    The default value is ``1000``.

:indices.recovery.translog_size:
    Specifies how much data of the transaction log should be
    transfered betweem shards in a single request during the recovery
    process. If ``indices.recovery.translog_op`` is reached first,
    value is ignored for this request.
    The default value is ``512kb``.

:indices.recovery.compress:
    Define if transferred data should be compressed during the
    recovery process. Defaults to ``true``. Setting it to ``false``
    may lower the pressure on the CPU while resulting in more data
    being transfererd over the network.

:indices.recovery.max_bytes_per_sec:
    Specifies the maximum number of bytes that can be transferred
    during shard recovery per seconds. Defaults to ``20mb``. Limiting
    can be disabled by setting it to ``0``. Similiar to
    ``indices.recovery.concurrent_streams`` this setting allows to
    control the network usage of the recovery process. Higher values
    may result in higher network utilization, but also faster recovery process.

Store Level Throttling
----------------------

:indices.store.throttle.type:
    Options are: ``all | merge | none``

    Allows to throttle ``merge`` (or ``all``) process of the store
    module. The default value is ``merge``.

:indices.store.throttle.max_bytes_per_sec:
    If throttling is enabled by ``indices.store.throttle.type``, this
    setting specifies the maximum bytes per second a store module
    process can operate with, defaults to ``20mb``.

Field Data Circuit Breaker
--------------------------

The field data circuit breaker allows estimation of needed memory
required for loading field data into memory. If a certain limit
is reached an exception is raised.

:indices.fielddata.breaker.limit:
    Specifies the limit for the fielddata breaker. The Default value
    is ``60%`` (of JVM heap).

:indices.fielddata.breaker.overhead:
    A constant that all field data estimations are multiplied with to
    determine a final estimation. The default value is ``1.03``.

Threadpools
-----------

Every node holds several thread pools to improve how threads are
managed within a node. There are several pools, but the important ones include:

    * index: For index/delete operations, defaults to fixed
    * search: For count/search operations, defaults to fixed
    * bulk: For bulk operations, defaults to fixed
    * refresh: For refresh operations, defaults to cache

:threadpool.<threadpool>.type:
    Options are: ``fixed | cache``

    ``fixed`` holds a fixed size of threads to handle the requests. It
    also has a queue for pending requests if no threads are available.

    ``cache`` will spawn a thread if there are pending requests (unbounded).

Fixed Threadpool Settings
~~~~~~~~~~~~~~~~~~~~~~~~~

If the type of a threadpool is set to ``fixed`` there are a few
optional settings.

:threadpool.<threadpool>.size:
    Number of threads. The default is number of cores times 5.

:threadpool.<threadpool>.queue_size:
    Size of the queue for pending requests. Default is -1 which is
    unbounded.

Metadata
--------

.. _conf_cluster_info_update_interval:

:cluster.info.update.interval:
    Defines how often the cluster collection metadata information
    (e.g. disk usages etc.) if no concrete  event is triggered. The
    default value is ``30s``.

Logging
=======

Crate Data comes, out of the box, with Log4j_ 1.2.x. It tries to simplify log4j
configuration by using YAML to configure it. The logging configuration
file is at ``config/logging.yml``.

The yaml file is used to prepare a set of properties used for logging configuration
using the `PropertyConfigurator`_ but without the tediously repeating ``log4j`` prefix.
Here is a small example of a working logging configuration::

    rootLogger: INFO, console

    logger:
      # log action execution errors for easier debugging
      action: DEBUG


    appender:
      console:
        type: console
        layout:
          type: consolePattern
          conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"

And here is a snippet of the generated properties ready for use with log4j. You get the point.
::

   log4j.rootLogger=INFO, console

   log4j.logger.action=DEBUG

   log4j.appender.console=org.elasticsearch.common.logging.log4j.ConsoleAppender
   log4j.appender.console.layout=org.apache.log4j.PatternLayout
   log4j.appender.console.layout.conversionPattern=[%d{ISO8601}][%-5p][%-25c] %m%n

   ...

Environment variables
=====================

.. _env-crate-home:

CRATE_HOME
----------

Specifies the home directory of the installation, it is used to find
default file paths like e.g. ``config/crate.yml`` or the default data
directory location. This variable is usally defined at the
by-distribution shipped start-up script. In most cases it is the
parent directory of the directory containing the ``bin/crate`` executable.

:CRATE_HOME: Home directory of Crate Data installation. Used to refer
             to default config files, data locations, log files, etc.
             All configured relative paths will use this directory as
             a parent.

CRATE_HEAP_SIZE
---------------

This variable specifies the amount of memory that can be used by the JVM. This
should be set to at least 50% of the machines memory.

Certain operations in Crate require a lot of records to be hold in memory at a
time. If the amount of heap that can be allocated by the JVM is too low these
operations would fail with an OutOfMemory exception.

The value of the environment variable can be suffixed with ``g`` or ``m``. For
example::

    CRATE_HEAP_SIZE=4g

.. _YAML: http://en.wikipedia.org/wiki/YAML

.. _Log4j: https://logging.apache.org/log4j/1.2/

.. _PropertyConfigurator: https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PropertyConfigurator.html
