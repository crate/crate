.. highlight:: sh
.. _cluster_upgrade:

=====================
Zero Downtime Upgrade
=====================


Crate provides an easy way to perform a cluster upgrade with zero
downtime.

To perform a rolling upgrade of a cluster one node at a time has to be stopped
using the ``graceful stop`` procedure.

This procedure will disable a node, which will cause it to reject any new
requests but will make sure that any pending requests are finished.

.. warning::

   Please make sure beforehand that your upgrade path is working
   i.e. that the old and new versions are compatible as they will run
   next to each other for the time of the upgrade process.
   Updating  `hotfix` versions is uncritically in most cases, but if you
   want to update `minor` or `major` versions you should restart the cluster
   instead of doing a Zero Downtime Upgrade to be 100% safe.
   See :ref:`versions <sys-versions>`.

.. note::

   Nonetheless, due to the distributed execution of requests, some client requests
   might fail during this procedure.

In addition it will ensure a certain data availability. Which can either be
``none``, ``primaries`` or ``full`` and can be configured using :ref:`ref-set`.

``full`` means that all shards that are currently located on the node
will be moved to the other nodes in order to stop gracefully.
Using this setting the cluster will stay ``green`` the
whole time.

``primaries`` means that only the primaries will be moved to other nodes.
Using this setting means that the cluster will go into the ``yellow``
warning state if a node that has been stopped contained replicas
that are then unavailable.

``none`` means that there is no data-availability guarantee.
The node will stop, possibly leaving the cluster in the critical ``red`` state
if the node contained a primary that has no replicas that can take over.

Requirements
============

Full Minimum Data Availability
------------------------------

If the ``full`` minimum data availability is configured the cluster needs to
contain enough nodes to hold the number of replicas that are configured even if
one node is missing.

So for example if there are only two nodes in a cluster and a table has one
replica configured the ``graceful stop`` procedure will not succeed
and abort as it won't be possible to relocate the replicas.

If a table has a range configured as number of replicas this will take into
account the upper number of replicas.

So again, with two nodes and a number of replicas of '0-1' will cause the
``graceful stop`` procedure to abort.

In short: In order for the ``full`` graceful stop to work the following has to be true::

    number_of_nodes > max_number_of_replicas + 1


Primaries Minimum Data Availability
-----------------------------------

If the ``primaries`` minimum data availability is used one has to take care
that there are still enough replicas in the cluster after a node has been
stopped that a write-consistency can be guaranteed.

By default write or delete operations only succeed if a quorum ( > replicas / 2
+ 1 ) of active shards is available.

.. note::

    If only 1 replica is configured one active shard suffices in
    order for write and delete operations to succeed.


Step 1: Disable Re-Allocations
==============================

First, you have to prevent the cluster from re-distributing shards and replicas
while certain nodes are not available. You can do that by disabling
re-allocations and only allowing new primary allocations.

Use the :ref:`ref-set` command to do so:

.. code-block:: psql

  cr> SET GLOBAL TRANSIENT cluster.routing.allocation.enable = 'new_primaries';
  SET OK (... sec)

.. note::

  This step may be omited if you set the ``graceful_stop.min_availability``
  setting to ``full``.


Step 2: Graceful Stop
=====================

The ``crate`` process supports multiple signals (see :ref:`cli_signals`) that invoke different
shutdown procedures. To stop a Crate process gracefully you need to
send the ``USR2`` signal::

  sh$ kill -USR2 `cat /path/to/crate.pid`

Depending on your installation you can make use of the init script.

E.g. like that::

  sh$ service crate graceful-stop

Depending on the size of your cluster this might take a while.
You might want to check your server logs to see, if the graceful stop
process is going well. In case of an error or a timeout, the node will stay up,
signalling the error in its log files (or wherever you put your log messages).

Using the default settings the node would shut down by moving all primary shards
off the node first. This will ensure that no data is lost. However, the cluster
health will go most likely to yellow, because replicas that lived on that node
will be missing.

If you want to ensure green health, you'd need to change the
``graceful_stop.min_availability`` setting to ``full``. This will move
all shards off the node before shutting down.

Keep in mind that reallocating shards might take some time depending
on the amount of shards and the amount and size of records (and/or blob data).
For that reason you should set the ``timeout`` setting to a reasonable
time. By default the shutdown process aborts and the cluster will start
distributing shards evenly again. If you want to force a shutdown after
the timeout, even if the reallocating is not finished, you can set the
``force`` setting to ``true``.

.. warning::

  A forced stop does not ensure the minimum data availability
  defined in the settings and may result in temporary or even permanent
  loss of data!

.. note::

  When using ``graceful_stop.min_availability=full`` there have to be
  enough nodes in the cluster to move shards
  or else the graceful shutdown procedure will fail!
  For example, if there are 4 nodes and 3 configured replicas, there will not be
  enough nodes to to fulfill the required replicas.

.. note::

  Also, if there is not enough disk space on other nodes to move the shards to
  the graceful stop procedure will fail.

You can also set the ``reallocate`` setting to ``false`` if you want to ensure
that a node cannot be stopped if the cluster would need to move shards off the
node. This can prevent long waiting times until the shutdown process runs
into timeout during reallocation.

By default, only the ``graceful stop`` command considers the cluster settings
described at :ref:`conf_graceful_stop`.

Observing the Reallocation
--------------------------

If you want to observe the reallocation process triggered by a
``full`` or ``primaries`` graceful-stop, you can issue the following sql queries
regularly.

Get the number of shards remaining on your deallocating node:

.. code-block:: psql

  cr> SELECT count(*) as remaining_shards from sys.shards
  ... where _node['name'] = 'your_node_name';
  +------------------+
  | remaining_shards |
  +------------------+
  |                0 |
  +------------------+
  SELECT 1 row in set (... sec)


Get some more detail about what shards are remaining on your node:

.. code-block:: psql

  cr> SELECT schema_name as schema, table_name as "table", id, "primary", state
  ... FROM sys.shards
  ... WHERE _node['name'] = 'your_node_name' AND schema_name IN ('blob', 'doc')
  ... ORDER BY schema, "table", id, "primary", state;
  +--------+-------+----+---------+-------+
  | schema | table | id | primary | state |
  +--------+-------+----+---------+-------+
  ...
  SELECT ... rows in set (... sec)

In case of ``primaries`` availability only the primary shards of tables with
0 replicas will be reallocated. Use this query to find out, for which shards to
look:

.. code-block:: psql

   cr> SELECT schema_name as schema, table_name as "table"
   ... FROM information_schema.tables
   ... WHERE number_of_replicas = 0 and schema_name in ('blob', 'doc')
   ... ORDER BY schema, "table" ;
   +--------+-------------------+
   | schema | table             |
   +--------+-------------------+
   ...
   +--------+-------------------+
   SELECT ... rows in set (... sec)


.. note::

   If you observe the graceful-stop process using the admin-ui, you might see
   the cluster turning red for a small instant when a node finally shuts down.
   This is due to the way the admin ui determines the cluster state. If a query
   fails due to a missing node, the admin-ui may falsely consider the cluster to
   be in a critical state.

Step 3: Upgrade Crate
=====================

After the node is stopped you can safely upgrade your Crate installation.
Depending on your installation and operating system you can do it by downloading
the latest tarball or just use the package manager.

Example for RHEL/YUM::

  $sh yum update -y crate

If you are in doubt how to upgrade an installed package,
please refer to the man pages of your operating system / package manager.

Step 4: Start Crate
===================

Once the upgrade process is completed you can start the Crate process again
by either invoking the bin/crate executable from the tarball directly::

  sh$ /path/to/bin/crate

or using the service manager of your operating system.

Example for RHEL/YUM::

  sh$ service crate start


Step 5: Repeat
==============

Repeat step 2, 3 and 4 for all other nodes.


Step 6: Enable Reallocations
============================

Last but not least when all nodes are updated you can re-enable
allocations again that have been disabled in the first step:

.. code-block:: psql

  cr> SET GLOBAL TRANSIENT cluster.routing.allocation.enable = 'all';
  SET OK (... sec)


.. warning::

  Even the safest upgrade process might go wrong at some point.
  So make sure you have your data backed up!

