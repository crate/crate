.. highlight:: psql
.. _copy_from:

=========
COPY FROM
=========

Copy data from files into a table.

Synopsis
========

::

    COPY table_ident [ PARTITION (partition_column = value [ , ... ]) ]
    FROM uri [ WITH ( option = value [, ...] ) ]

where `option` can be one of:

- `bulk_size` *integer*
- `shared` *boolean*
- `num_readers` *integer*
- `compression` *string*
- `overwrite_duplicates` *boolean*

Description
===========

COPY FROM copies data from a URI to the specified table.

The nodes in the cluster will attempt to access the resources available under
the URI and import the data.

The file(s) must contain one JSON formatted row per line and have to be encoded
using UTF-8. Empty lines are skipped.

For examples see: :ref:`importing_data`.

.. note::

  When importing data using ``COPY FROM`` Crate does not check if the types
  from the columns and the types from the import file match. It does not cast
  the types to their target but will always import the data as it is
  in the source file.

  e.g. a `WKT`_ string cannot be imported into a column of ``geo_shape`` or
  ``geo_point`` type, such as it won't be implicitly casted to `GeoJSON`_
  format.


URI
===

The URI must be formatted according to the `URI Scheme`_.

In case the scheme name is missing it is assumed that the value is a file-path
instead and will be converted to an URI implicitly.


For example:

.. code-block:: text

    /tmp folder/file.json

Is converted to:

.. code-block:: text

    'file:///tmp%20folder/file.json'


Supported schemes
-----------------

file
^^^^

The filepath given must be an absolute path and accessable by the crate
process.

By default each node will attempt to read the files specified. If the URI
points to a shared folder the ``shared`` option must be set to true in order to
avoid duplicate imports.


.. _copy_from_s3:

s3
^^

Can be used to access buckets on the Amazon AWS S3 Service:

.. code-block:: text

    s3://[<accesskey>:<secretkey>@]<bucketname>/<path>

If the accesskey and secret key is ommited an attempt to load the credentials
from the environment or java settings is made.

Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY
Java System Properties - aws.accessKeyId and aws.secretKey

If no credentials are set the s3 client will operate in anonymous mode, see
`AWS Java Documentation`_.

Using a s3 URI will set the ``shared`` option implicitly.

.. note::

   A ``secretkey`` provided by Amazon Web Service can contain characters such
   as '/', '+' or '='. Such characters must be URI encoded. For example, if
   the authentication code includes a plus (+) sign, encode it as %2B in the
   request. Encode a forward slash as %2F and equals as %3D. For a detailed
   explanation read the official `AWS documentation`_.

.. note::

  Versions prior to 0.51.x use HTTP for connections to S3. Since 0.51.x these
  connections are using the HTTPS protocol. Please make sure you update your
  firewall rules to allow outgoing connections on port ``443``.

Parameters
==========

:table_ident: The name (optionally schema-qualified) of an existing
    table where the data should be put.

:uri: An expression which evaluates to a uri as defined in `RFC2396`_. The
      supported schemes are listed above. The last part of the path may also
      contain ``*`` wildcards to match multiple files.

PARTITION Clause
================

If the table is partitioned this clause can be used to import data into a
specific partition. This clause takes one or more partition columns and for
each column a value.

::

    [ PARTITION ( partition_column = value [ , ... ] ) ]


:partition_column: The name of the column by which the table is partitioned.
                   All partition columns that were part of the
                   :ref:`partitioned_by_clause` of the :ref:`ref-create-table`
                   statement must be specified.

:value: The columns value.

In a table that is partitioned the values for the columns by which the
table is partitioned are not actually stored with each row.

The files that are imported should *not* contain values for the partitioned
columns as the ``COPY FROM ... PARTITION`` statement does not check that and
will unnecessarily import them.

WITH Clause
===========

The optional ``WITH`` clause can specify options for the COPY FROM statement.

::

    [ WITH ( option = value [, ...] ) ]

Options
-------

bulk_size
^^^^^^^^^

Crate will process the lines it reads from the ``path`` in bulks. This option
specifies the size of such a bulk. The default is 10000.

Must be set to a number greater than 0

shared
^^^^^^

This option should be set if the URI points to a shared storage. It will
prevent multiple nodes/readers from importing the same file.

The default value depends on the used URI scheme.

num_readers
^^^^^^^^^^^

The number of nodes that will read the resources specified in the URI. Defaults
to the number of nodes available in the cluster. If the option is set to a
number greater than the number of available nodes it will still only use each
node only once to do the import.

Must be an integer that is greater than 0.

compression
^^^^^^^^^^^

The default value is ``null``. Can be set to ``gzip`` to read gzipped files.

overwrite_duplicates
^^^^^^^^^^^^^^^^^^^^

Default: false

``COPY FROM`` by default won't overwrite rows if a document with the same
primary key already exists.

If this option is set to true this behaviour is changed so that documents will
be overwritten.

.. _`AWS documentation`: http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html

.. _`AWS Java Documentation`: http://docs.aws.amazon.com/AmazonS3/latest/dev/AuthUsingAcctOrUserCredJava.html

.. _`RFC2396`: http://www.ietf.org/rfc/rfc2396.txt

.. _`URI Scheme`: https://en.wikipedia.org/wiki/URI_scheme

.. _GeoJSON: http://geojson.org/

.. _WKT: http://en.wikipedia.org/wiki/Well-known_text
