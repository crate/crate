.. highlight:: psql
.. _copy_to:

=======
COPY TO
=======

Export table contents to files on crate node machines.

Synopsis
========

::

    COPY table_ident [ PARTITION ( partition_column = value [ , ... ] ) ]
                     [ ( column [ , ...] ) ]
                     TO [DIRECTORY] output_uri
                     [ WITH ( copy_parameter [= value] [, ... ] ) ]

Description
===========

Copy the contents of a table to one or many files on any cluster node
containing data from the given table.

.. note::

  Output files will always be stored on the cluster node machines, not
  on the client machine.

The created files are JSON formatted and contain one table row per
line.

If the ``DIRECTORY`` keyword is given, the uri is treated as a directory path.
This will generate one or more files in the given directory, named in such a
way to prevent filename conflicts.

.. note::

 Data is written per shard, so if there is more than one shard of the
 exported table on the same node, the output file will get
 corrupted due to concurrent writes to the same file. The same behaviour
 holds true when Crate exports data to shared storage systems such
 as `Amazon S3`_ or `NFS`_.

 To prevent such cases use the ``DIRECTORY`` keyword. An example is given
 in :ref:`exporting_data`.


.. note::

  Currently only user tables can be exported. System tables like `sys.nodes`
  and blob tables don't work with the `COPY TO` statement.

Parameters
==========

:table_ident: The name (optionally schema-qualified) of the table to
  be exported.

:column: (optional) A list of column expressions that should be exported.

.. note::

    Declaring columns changes the output to JSON list format, which is
    currently not supported by the COPY FROM statement.

Output URI
==========

The ``output_uri`` can be any expression evaluating to a string.
The resulting string should be a valid URI of one of the supporting schemes:

 * ``file://``
 * ``s3://[<accesskey>:<secretkey>@]<bucketname>/<path>``

If no scheme is given (e.g.: '/path/to/file') the default uri-scheme ``file://``
will be used.

.. note::

    If the s3 scheme is used without specifying any credentials an attempt is
    made to read these information from the AWS_ACCESS_KEY_ID and
    AWS_SECRET_KEY environment variables. In addition to that the Java System
    properties aws.accessKeyId and aws.secretKey are also used as a fallback.

.. note::

   A ``secretkey`` provided by Amazon Web Service can contain characters such
   as '/', '+' or '='. Such characters must be URI encoded. The same encoding
   as in :ref:`copy_from_s3` applies.

.. note::

  Versions prior to 0.51.x use HTTP for connections to S3. Since 0.51.x these
  connections are using the HTTPS protocol. Please make sure you update your
  firewall rules to allow outgoing connections on port ``443``.

PARTITION Clause
================

If the table is partitioned this clause can be used to only export data from a
specific partition.

The exported data doesn't contain the partition columns or values as they are
not part of the partitioned tables.

::

    [ PARTITION ( partition_column = value [ , ... ] ) ]


:partition_column: The name of the column by which the table is partitioned.
                   All partition columns that were part of the
                   :ref:`partitioned_by_clause` of the :ref:`ref-create-table`
                   statement must be specified.

:value: The columns value.

.. note::

    If ``COPY TO`` is used on a partitioned table without the ``PARTITION``
    clause the partition columns and values will be included in the rows of the
    exported files.

WITH Clause
===========

The optional WITH clause can specify parameters for the copy statement.

::

    [ WITH ( copy_parameter [= value] [, ... ] ) ]

Possible copy_parameters are:

.. _compression:
..

compression
-----------

Define if and how the exported data should be compressed.
Per default no compression is applied.

Possible values for the ``compression`` setting are:

:gzip: The exported data is compressed with gzip_.

.. _gzip: http://www.gzip.org/

.. _`Amazon S3`: http://aws.amazon.com/s3/

.. _NFS: http://de.wikipedia.org/wiki/Network_File_System


