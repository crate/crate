.. highlight:: psql
.. _copy_to:

=======
COPY TO
=======

Export table contents to files on crate node machines.

Synopsis
========

::

    COPY table_ident [ PARTITION ( partition_column = value [ , ... ] ) ]
                     [ ( column [ , ...] ) ]
                     [ WHERE condition ]
                     TO [DIRECTORY] output_uri
                     [ WITH ( copy_parameter [= value] [, ... ] ) ]

Description
===========

The ``COPY TO`` command exports the contents of a table to one or more files.
Each node of the cluster which contains a shard of the table will export the
contents of its shards.

The created files are JSON formatted and contain one table row per line.

If the ``DIRECTORY`` keyword is given, the uri is treated as a directory path.
This will generate one or more files in the given directory, named in such a
way to prevent filename conflicts.



Implications of shard level export
==================================

Exporting the data on a shard level is the fastest possible way to export data
in Crate.

But there are a few quirks that users have to be aware of when using ``COPY TO``:

Since data is exported and written per shard there might be concurrent writes
to the same file if a single node contains more than one shard. This could
cause corruption in the file that is being written. The same also holds true
when data is being written to a shared file system such as `Amazon S3`_ or
`NFS`_.

In order to prevent this from happening it is required to write to a separate
file per shard. To do this the ``DIRECTORY`` keyword can be used. An example is
given in :ref:`exporting_data`.


.. note::

  Currently only user tables can be exported. System tables like `sys.nodes`
  and blob tables don't work with the `COPY TO` statement.


Parameters
==========

:table_ident: The name (optionally schema-qualified) of the table to
  be exported.

:column: (optional) A list of column expressions that should be exported.

.. note::

    Declaring columns changes the output to JSON list format, which is
    currently not supported by the COPY FROM statement.

WHERE Clause
------------

`WHERE` clauses follow the same syntax as used in `SELECT` statements (see
:ref:`sql_dql_where_clause` for more information).


Output URI
==========

The ``output_uri`` can be any expression evaluating to a string.
The resulting string should be a valid URI of one of the supporting schemes:

 * ``file://``
 * ``s3://[<accesskey>:<secretkey>@]<bucketname>/<path>``

If no scheme is given (e.g.: '/path/to/file') the default uri-scheme ``file://``
will be used.

.. note::

    If the s3 scheme is used without specifying any credentials an attempt is
    made to read these information from the AWS_ACCESS_KEY_ID and
    AWS_SECRET_KEY environment variables. In addition to that the Java System
    properties aws.accessKeyId and aws.secretKey are also used as a fallback.

.. note::

   A ``secretkey`` provided by Amazon Web Service can contain characters such
   as '/', '+' or '='. Such characters must be URI encoded. The same encoding
   as in :ref:`copy_from_s3` applies.

.. note::

  Versions prior to 0.51.x use HTTP for connections to S3. Since 0.51.x these
  connections are using the HTTPS protocol. Please make sure you update your
  firewall rules to allow outgoing connections on port ``443``.

PARTITION Clause
================

If the table is partitioned this clause can be used to only export data from a
specific partition.

The exported data doesn't contain the partition columns or values as they are
not part of the partitioned tables.

::

    [ PARTITION ( partition_column = value [ , ... ] ) ]


:partition_column: The name of the column by which the table is partitioned.
                   All partition columns that were part of the
                   :ref:`partitioned_by_clause` of the :ref:`ref-create-table`
                   statement must be specified.

:value: The columns value.

.. note::

    If ``COPY TO`` is used on a partitioned table without the ``PARTITION``
    clause the partition columns and values will be included in the rows of the
    exported files.

WITH Clause
===========

The optional WITH clause can specify parameters for the copy statement.

::

    [ WITH ( copy_parameter [= value] [, ... ] ) ]

Possible copy_parameters are:

.. _compression:
..

compression
-----------

Define if and how the exported data should be compressed.
Per default no compression is applied.

Possible values for the ``compression`` setting are:

:gzip: The exported data is compressed with gzip_.

.. _gzip: http://www.gzip.org/

.. _`Amazon S3`: http://aws.amazon.com/s3/

.. _NFS: http://de.wikipedia.org/wiki/Network_File_System


